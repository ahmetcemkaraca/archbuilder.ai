name: Performance Gates
# CI Performance Gates - P42-T3

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]  # GitFlow: PRs to both main and develop
    types: [ opened, synchronize, reopened ]
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM

jobs:
  performance-gates:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: archbuilder_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        cd src/cloud-server
        pip install -r requirements.txt
        pip install locust==2.17.0
        pip install pytest-benchmark==4.0.0
        pip install pytest-xdist==3.6.0
        
    - name: Start application
      run: |
        cd src/cloud-server
        python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 &
        sleep 10
        
    - name: Wait for application
      run: |
        timeout 60 bash -c 'until curl -f http://localhost:8000/v1/health; do sleep 2; done'
        
    - name: Run performance benchmarks
      run: |
        cd src/cloud-server
        python -m pytest tests/load_tests.py::test_load_test_suite -v --tb=short
        
    - name: Run load tests
      run: |
        cd src/cloud-server
        python tests/load_tests.py --duration 60 --users 50
        
    - name: Performance validation
      run: |
        cd src/cloud-server
        python -c "
        import json
        import sys
        
        # Load performance results
        try:
            with open('performance_results.json', 'r') as f:
                results = json.load(f)
        except FileNotFoundError:
            print('Performance results not found')
            sys.exit(1)
        
        # Performance thresholds
        thresholds = {
            'max_response_time_ms': 2000,
            'max_error_rate': 0.05,
            'min_throughput_rps': 10,
            'ai_max_response_time_ms': 30000,
            'ai_max_error_rate': 0.1
        }
        
        # Validate results
        failed_checks = []
        
        for test_name, test_results in results.items():
            print(f'Validating {test_name}...')
            
            # Check response time
            if test_results['avg_response_time_ms'] > thresholds['max_response_time_ms']:
                failed_checks.append(f'{test_name}: Response time {test_results[\"avg_response_time_ms\"]}ms exceeds {thresholds[\"max_response_time_ms\"]}ms')
            
            # Check error rate
            if test_results['error_rate'] > thresholds['max_error_rate']:
                failed_checks.append(f'{test_name}: Error rate {test_results[\"error_rate\"]:.2%} exceeds {thresholds[\"max_error_rate\"]:.2%}')
            
            # Check throughput
            if test_results['requests_per_second'] < thresholds['min_throughput_rps']:
                failed_checks.append(f'{test_name}: Throughput {test_results[\"requests_per_second\"]:.2f} RPS below {thresholds[\"min_throughput_rps\"]} RPS')
        
        if failed_checks:
            print('Performance validation failed:')
            for check in failed_checks:
                print(f'  ❌ {check}')
            sys.exit(1)
        else:
            print('✅ All performance checks passed')
        "
        
    - name: Generate performance report
      if: always()
      run: |
        cd src/cloud-server
        python -c "
        import json
        import os
        from datetime import datetime
        
        # Create performance report
        report = {
            'timestamp': datetime.utcnow().isoformat(),
            'branch': os.environ.get('GITHUB_REF', 'unknown'),
            'commit': os.environ.get('GITHUB_SHA', 'unknown'),
            'performance_summary': {
                'tests_run': 0,
                'tests_passed': 0,
                'tests_failed': 0,
                'overall_status': 'unknown'
            }
        }
        
        # Try to load results
        try:
            with open('performance_results.json', 'r') as f:
                results = json.load(f)
                report['performance_summary']['tests_run'] = len(results)
                report['performance_summary']['tests_passed'] = len([r for r in results.values() if r.get('passed', False)])
                report['performance_summary']['tests_failed'] = len([r for r in results.values() if not r.get('passed', True)])
        except FileNotFoundError:
            report['performance_summary']['overall_status'] = 'no_results'
        
        # Save report
        with open('performance_report.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        print('Performance report generated')
        "
        
    - name: Upload performance artifacts
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: performance-results
        path: |
          src/cloud-server/performance_results.json
          src/cloud-server/performance_report.json
          src/cloud-server/locust_report.html
        retention-days: 30

  performance-monitoring:
    runs-on: ubuntu-latest
    needs: performance-gates
    if: github.event_name == 'schedule'  # Only run on scheduled builds
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        
    - name: Install monitoring dependencies
      run: |
        pip install prometheus-client==0.19.0
        pip install grafana-api==1.0.3
        
    - name: Run performance monitoring
      run: |
        python -c "
        import time
        import json
        from datetime import datetime, timedelta
        
        # Simulate performance monitoring
        monitoring_data = {
            'timestamp': datetime.utcnow().isoformat(),
            'metrics': {
                'cpu_usage': 45.2,
                'memory_usage': 67.8,
                'response_time_p95': 1250,
                'response_time_p99': 2100,
                'error_rate': 0.02,
                'throughput_rps': 15.5
            },
            'alerts': []
        }
        
        # Check for performance degradation
        if monitoring_data['metrics']['response_time_p95'] > 2000:
            monitoring_data['alerts'].append('High response time detected')
        
        if monitoring_data['metrics']['error_rate'] > 0.05:
            monitoring_data['alerts'].append('High error rate detected')
        
        # Save monitoring data
        with open('monitoring_data.json', 'w') as f:
            json.dump(monitoring_data, f, indent=2)
        
        print('Performance monitoring completed')
        "
        
    - name: Upload monitoring data
      uses: actions/upload-artifact@v3
      with:
        name: monitoring-data
        path: monitoring_data.json
        retention-days: 90


